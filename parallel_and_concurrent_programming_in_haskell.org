#+Title: Parallel and Concurrent Programming in Haskell: Summary
#+Author: Anton Logvinenko
#+Email: anton.logvinenko@gmail.com
#+latex_header: \hypersetup{colorlinks=true,linkcolor=blue}
#+latex_header: \usepackage{parskip}
#+latex_header: \linespread{1}
#+MACRO: PB @@latex:\pagebreak@@ @@html: <br/><br/><br/><hr/><br/><br/><br/>@@ @@ascii: |||||@@
#+LATEX_HEADER: \usepackage[margin=0.75in]{geometry}

{{{PB}}}

* Intro
This is a summary of Simon Marlow's book [[https://simonmar.github.io/pages/pcph.html]["Parallel and Concurrent Prorgamming in Haskell"]].

Free online version is available [[https://www.oreilly.com/library/view/parallel-and-concurrent/9781449335939/][here]].


{{{PB}}}

* Concurrency and Parallelism
*Parallel program* uses a multiplicity of hardware to perform a computation more quickly: *to arrive at the answer earlier*.
Alternatives: better algorithm, lower quality, better hardware.
*Concurrency*: program-structuring technique in which there are *multiple threads of control* to
*interact with multiple independent external agents*.
Conceptually, threads run at the same time whether they actually execute at the same time is an implementation detail.
Alternatives: event loops, callbacks.

*Detetrministic* programming model: a program can give only one result. *Non-deterministic* programming model: a program may have many results.

*Concurrent* programming models are *necessarily non-deterministic*. They interact with external agents that cause events at unpredictable times.
*Most parallel* programming models *can be deterministic*. Exceptions:
 - some algorithms depend on internal non-determinism
 - when you want to parallelize programs that do have side effects

*In Haskell*, concurrency (in general) is a structuring technique for effectful code.
Pure code has no effects to observe & evaluation order doesn't matter.

{{{PB}}}

* Parallel Haskell
*Automatic parallelization problem*: to make program faster we need to gain more from parallelism than we lose due to overhead.
*Alternative*: use profiling to find candidates for parallelism. Parallel programs in Haskell *elimitate* some difficulties:
 - parallel programming is deterministic in Haskell
 - use of high-level & declarative models, without having to deal with synchronization and communication: programmer indicates where the parallelism is & the RTS will handle the details of running program in parallel
  - programs are abstract and likely to work on more hardware
  - takes advantage of RTS GC
  - but performance problems can be hard to understand

The main thing to think about in parallel Haskell is *partitioning*:
 - *granularity*: large enough to dwarf overhead, but not too large to keep all CPUs busy
 - *data dependencies*: when one task depends on another htye must be executed sequentially
   - *implicit data dependencies* are more concise, but it may be more difficult to reason about performance and fix problems
   - *explicit data dependencies* are less concise but easier to analyze

{{{PB}}}

** Basic Parallelism: the Eval Monad
*** Lazy Evaluation, Weak Head Normal Form
Relevant commands:
 - =:sprint= prints value without causing it to be evaluated
 - =seq a b= evals a to WHNF, then returns *b*

General principles:
 - defining an expression causes a /thunk/ to be built
 - a thunk remains /unevaluated/ until the value is required
 - once evaluated, the thunk is /replaced/ by a value

An expression is in WHNF if it's either:
 - a constructor: =True=, =(:) 1=
 - lambda abstraction: =\x -> expression=
 - built-in function applied to too few arguments: =(+) 2=
Exception: fully applied constructor for a datatype with some fields declared strict.

How to test whether in WHNF:
 * =:sprint x=
 * =seq x ()=
 * =:sprint x=
 * If =:sprtin x= gives identical results then *x* was in WHNF

*** The Eval Monad, rpar, and rseq
#+BEGIN_SRC Haskell
data Eval a
instance Monad Eval

runEval :: Eval a -> a
rpar    :: a -> Eval a   ;;evaluate in parallel, don't wait
rseq    :: a -> Eval a   ;;evaluate & wait
#+END_SRC

Typical use
 if we expect to generate *more parallelism* soon or if we *don't depend on the result* of either operations
#+BEGIN_SRC Haskell
runEval $ do
   a <- rpar (f x)
   b <- rpar (f y)
   return (a, b)
#+END_SRC

Typical use if we *generated all the parallelism* we need or if we *depend on results* of operations:
#+BEGIN_SRC Haskell
runEval $ do
   a <- rpar (f x)
   b <- rpar (f y)
   rseq a
   rseq b
   return (a, b)
#+END_SRC

*** ThreadScope, compiler & executable options for parallelism
Compiler:
 - =eventlog= enable =-l= option for binary
 - =threaded= compile with parallelis,
 - =rtsoprts= enable +RTS option for binary
Executable:
 - =+RTS= starts passing RTS flags
 - =-RTS= closes sequemce of RTS flags (optional if nothing goes after them)
 - =+RTS -s= display statistics
 - =-RTS -l= generate log that can be opened with ThreadScope

*** GHC dynamic partitioning
GHC sparks:
 - =rpar= argument is a *spark*
 - sparks are collected in a pool
 - sparks are taken from pool when processors are available
 - RTS uses work stealing to execute sparks
GHC spark can be in following states:
 - *converted* into real parallism
 - *overflowed* pool of limited size was overflowed, sparks dropped
 - *dud*: rpar was applied to already evaluated expression
 - *GC'd*: spark was found to be unused by the program
 - *fizzled*: unevaluated when pased to rpar, but evaluated later (dropped from pool)

*** Amdahl's Law
*P*: portion of the program that can be parallelized

*N*: number of available processors

Then the optimal work layout is defined as:
\begin{equation}
(1-P)+P/N
\end{equation}

And theoretically possible speedup is:
\begin{equation}
\frac{1}{(1-P) + \frac{P}{N}}
\end{equation}

*** Tools to evaluate in WHNF
 Evaluate *a* to WHNF, then return *b*
#+BEGIN_SRC
seq a b :: a -> b -> b
#+END_SRC
 Evaluate *a* to WHNF in IO:
#+BEGIN_SRC
evaluate :: a -> IO a
#+END_SRC

*** Tools to evaluate in NF
Let's introduce a special class type:
#+BEGIN_SRC
class NFData where
   rnf :: a -> ()
   rnf a = a `seq` ()
#+END_SRC
It defaults to =seq= behavior which is fine for structures like =Bool=:
#+BEGIN_SRC
instance NFData Bool   ;;and many others in Control.Deepseq
#+END_SRC
Here's how to define instances for more complex datatypes:
#+BEGIN_SRC
instance NFData a => NFData (Tree a) where
   rnf Empty = ()
   rnf (Branch l a r) = rnf l `seq` rnf a `seq` rnf r
#+END_SRC
Higher level functions based on =NFData=:
#+BEGIN_SRC
deepseq :: NFData a => a -> b -> b   ;;like seq but for NF, not WHNF
deepseq a b = rnf a `seq` b

force :: NFData => a -> a
force x  = x `deepseq` x
#+END_SRC

*** NF/WHNF summary
Evaluation to varying degrees is possible:
 - *WHNF*, O(1), weak evaluation
 - *NF*, O(n), deep evaluation (traverses the whole structure)

{{{PB}}}

** Evaluation Strategies

*** Strategies
Let's define the following type:
#+BEGIN_SRC
type Strategy a = a -> Eval a
#+END_SRC

Now we can speculate that =rpar= and =rseq= are also strategies:
#+BEGIN_SRC
rpar :: Strategy a
rseq :: Strategy a
#+END_SRC

We can introduce a little helper function:
#+BEGIN_SRC
using :: a -> Strategy a -> a
x `using` s = runEval (s x)
#+END_SRC

Now we can define higher level strategies:
#+BEGIN_SRC
parPair :: Strategy (a, b)
parPair (a, b) = do
   a' <- rpar a
   b' <- rpar b
   return (a', b')
#+END_SRC

We can use =parPair= stratey:
#+BEGIN_SRC
runEval (runPair(fib 35, fib 36))
#+END_SRC

Or if we rewrite with =using=:
#+BEGIN_SRC
(fib 35, fib 36) `using` parPair
#+END_SRC

*** Parameterized Strategies
We can define functions that build new strategies using existing ones.

First, let's make strategy =evalPair= for pair evaluation that is customizable by separate strategies for its components:
#+BEGIN_SRC
evalPair :: Strategy a -> Strategy b -> Strategy (a, b)
evalPair sa sb (a, b) = do
   a' <- sa a
   b' <- sb b
   return (a', b')
#+END_SRC
Second, let's define strategy =parPair= for parallel pair evaluation that is customizable by separate strategies for its components.
But first let's look at =rparWith= strategy that runs evaluation with supplied strategy but in parallel:
#+BEGIN_SRC
rparWith :: Strategy :: Strategy a -> Strategy a
rparWith strat = parEval . strat
#+END_SRC
Now let's use =rparWith= and =evalPair= to define =parPair=:
#+BEGIN_SRC
parPair :: Strategy a -> Straetgy b -> Strategy (a, b)
parPair sa sb = evalPair (rparWith sa) (rparWith sb)
#+END_SRC
Third, let's look at =rdeepseq= function:
#+BEGIN_SRC
rdeepseq :: NFData a => Strategy a
rdeepseq x = rseq (force x)
#+END_SRC
Now let's use it to build the final strategy:
#+BEGIN_SRC
parPair rdeepseq rdeepseq :: (NFData a, NFData b) => Strategy (a, b)
#+END_SRC
We build a strategy that deeply evaluates pair components in parallel by doing following steps:
 - Defined =evalPair=
 - =evalPair= with =rparWith= gave us =parPair=
 - =parPair= with =rdeepseq= gave us the final strategy
Let's use one more function, =r0=:
#+BEGIN_SRC
r0 :: Strategy a
r0 x = return x
#+END_SRC
Function =r0= avoids evaluation. Let's build a strategy that doesn't evaluate second components of a pair of pairs:
#+BEGIN_SRC
evalPair (evalPair rpar r0) (evalPair rpar r0) :: Strategy ((a, b), (a, b))
#+END_SRC

*** Speculative parallelism
Let's consider the following =evalList= implementations:
#+BEGIN_SRC
parList1 :: Strategy a -> Strategy [a]
parList1 stat = evalList (rparWith strat)

evalList :: Strategy a -> Strategy [a]
evalList strat [] = return []
evalList strat (x:xs) = do
   x' <- strat x
   xs <- evalList strat xs
   return (x':xs')

parList2 :: Strategy a -> Strategy [a]
parList2 strat xs = do
   go xs
   return xs
where
   go []     = return ()
   go (x:xs) = do rparWith strat x
                  go xs
#+END_SRC

Impotant thing to note is that in =parList1= we're building a new list. It might look like we might just generate sparks that would evaluate list items in parallel, as we did in =parList2=.
But this assumption is not true: if we only generate sparks, then only pool will reference them and they would be GC'd, i.e. we'd witness *speculative parallelism*.
Since we don't want it in this particular case, we need to have something else to reference sparks, hence building the resulting list: =parList1= is the correct implementation, =parList2= is not.

To summarize, a *bad use* of strategy looks like:
#+BEGIN_SRC
do
   ...
   rpar (f x)
   ...
#+END_SRC
*Good use* of stragy looks like:
#+BEGIN_SRC
do
   ...
   y <- rpar (f x)
   ... y ...
#+END_SRC
Or like this, if *y* is used somewhere in the program:
#+BEGIN_SRC
do
   ...
   rpar y
   ...
#+END_SRC

*** Practical considerations
 - *Spark generation* will be likely done on different cores (switching cores)
 - *Granularity*
  - Generate enough work to make CPUs busy
  - But not too much
   - Overhead per chunk: creating, running
   - Amount of sequential work increases: need to merge results

*** Higher level functions
=parBuffer= creates sparks only for *N* first elements of list and keeps the number of sparks equal to *N* when some are evaluated:
#+BEGIN_SRC
parBuffer :: Int -> Strategy a -> Strategy [a]
#+END_SRC

=parListChunk= creates chunks of *N* elements each:
#+BEGIN_SRC
parListChunk :: Int -> Strategy a -> Strategy [a]
#+END_SRC

=withStrategy= is an alias for =using=:
#+BEGIN_SRC
withStrategy s x == x `using` s
#+END_SRC

*** The Identity Property
The value strategy returns must be equal to the value it was passed:
#+BEGIN_SRC
x `using` s == x
#+END_SRC
But there's a *caveat* though: =x `using` s= might be less defined than x, because it might evaluate more structure of x.
For example, compare how following expressions would be evaluated:
#+BEGIN_SRC
print $ snd (1 `div` 0, "Hello!")
print $ snd ((1 `div` 0, "Hello!") `using` rdeepseq)
#+END_SRC

{{{PB}}}

** Dataflow Pararllelism: The Par Monad

*** Par Monad

*** Dataflow Parallelism

*** Pipeline Parallelism & Limiting the Producer

*** Using Different Schedulers
