#+Title: Parallel and Concurrent Programming in Haskell: Summary
#+Author: Anton Logvinenko
#+Email: anton.logvinenko@gmail.com
#+latex_header: \hypersetup{colorlinks=true,linkcolor=blue}
#+latex_header: \usepackage{parskip}
#+latex_header: \linespread{1}
#+MACRO: PB @@latex:\pagebreak@@ @@html: <br/><br/><br/><hr/><br/><br/><br/>@@ @@ascii: |||||@@
#+LATEX_HEADER: \usepackage[margin=0.75in]{geometry}

{{{PB}}}

* Intro
This is a summary of Simon Marlow's book [[https://simonmar.github.io/pages/pcph.html]["Parallel and Concurrent Prorgamming in Haskell"]].

Free online version is available [[https://www.oreilly.com/library/view/parallel-and-concurrent/9781449335939/][here]].

An =*.org= file containing sources of this document can be found [[https://github.com/antonlogvinenko/notes/blob/master/parallel_and_concurrent_programming_in_haskell.org][here]].

This book doesn't cover following topics:
- parallel programming with Repa and Accelerate
- distributed programming


{{{PB}}}

* Concurrency and Parallelism
*Parallel program* uses a multiplicity of hardware to perform a computation more quickly: *to arrive at the answer earlier*.
Alternatives: better algorithm, lower quality, better hardware.
*Concurrency*: program-structuring technique in which there are *multiple threads of control* to
*interact with multiple independent external agents*.
Conceptually, threads run at the same time whether they actually execute at the same time is an implementation detail.
Alternatives: event loops, callbacks.

*Detetrministic* programming model: a program can give only one result. *Non-deterministic* programming model: a program may have many results.

*Concurrent* programming models are *necessarily non-deterministic*. They interact with external agents that cause events at unpredictable times.
*Most parallel* programming models *can be deterministic*. Exceptions:
 - some algorithms depend on internal non-determinism
 - when you want to parallelize programs that do have side effects

*In Haskell*, concurrency (in general) is a structuring technique for effectful code.
Pure code has no effects to observe & evaluation order doesn't matter.

{{{PB}}}

* Parallel Haskell
*Automatic parallelization problem*: to make program faster we need to gain more from parallelism than we lose due to overhead.
*Alternative*: use profiling to find candidates for parallelism. Parallel programs in Haskell *elimitate* some difficulties:
 - parallel programming is deterministic in Haskell
 - use of high-level & declarative models, without having to deal with synchronization and communication: programmer indicates where the parallelism is & the RTS will handle the details of running program in parallel
  - programs are abstract and likely to work on more hardware
  - takes advantage of RTS GC
  - but performance problems can be hard to understand

The main thing to think about in parallel Haskell is *partitioning*:
 - *granularity*: large enough to dwarf overhead, but not too large to keep all CPUs busy
 - *data dependencies*: when one task depends on another htye must be executed sequentially
   - *implicit data dependencies* are more concise, but it may be more difficult to reason about performance and fix problems
   - *explicit data dependencies* are less concise but easier to analyze

{{{PB}}}

** Data Parallelism: the Eval Monad
*** Lazy Evaluation, Weak Head Normal Form
Relevant commands:
 - =:sprint= prints value without causing it to be evaluated
 - =seq a b= evals a to WHNF, then returns *b*

General principles:
 - defining an expression causes a /thunk/ to be built
 - a thunk remains /unevaluated/ until the value is required
 - once evaluated, the thunk is /replaced/ by a value

An expression is in WHNF if it's either:
 - a constructor: =True=, =(:) 1=
 - lambda abstraction: =\x -> expression=
 - built-in function applied to too few arguments: =(+) 2=
Exception: fully applied constructor for a datatype with some fields declared strict.

How to test whether in WHNF:
 * =:sprint x=
 * =seq x ()=
 * =:sprint x=
 * If =:sprtin x= gives identical results then *x* was in WHNF

*** The Eval Monad, rpar, and rseq
#+BEGIN_SRC Haskell
data Eval a
instance Monad Eval

runEval :: Eval a -> a
rpar    :: a -> Eval a   ;;evaluate in parallel, don't wait
rseq    :: a -> Eval a   ;;evaluate & wait
#+END_SRC

Typical use
 if we expect to generate *more parallelism* soon or if we *don't depend on the result* of either operations
#+BEGIN_SRC Haskell
runEval $ do
   a <- rpar (f x)
   b <- rpar (f y)
   return (a, b)
#+END_SRC

Typical use if we *generated all the parallelism* we need or if we *depend on results* of operations:
#+BEGIN_SRC Haskell
runEval $ do
   a <- rpar (f x)
   b <- rpar (f y)
   rseq a
   rseq b
   return (a, b)
#+END_SRC

*** ThreadScope, compiler & executable options for parallelism
Compiler:
 - =eventlog= enable =-l= option for binary
 - =threaded= compile with parallelis,
 - =rtsoprts= enable +RTS option for binary
Executable:
 - =+RTS= starts passing RTS flags
 - =-RTS= closes sequemce of RTS flags (optional if nothing goes after them)
 - =+RTS -s= display statistics
 - =-RTS -l= generate log that can be opened with ThreadScope

*** GHC dynamic partitioning, Amdahl's Law
GHC sparks:
 - =rpar= argument is a *spark*
 - sparks are collected in a pool
 - sparks are taken from pool when processors are available
 - RTS uses work stealing to execute sparks
GHC spark can be in following states:
 - *converted* into real parallism
 - *overflowed* pool of limited size was overflowed, sparks dropped
 - *dud*: rpar was applied to already evaluated expression
 - *GC'd*: spark was found to be unused by the program
 - *fizzled*: unevaluated when pased to rpar, but evaluated later (dropped from pool)

Amdahl's Law explains how much parallelism is theoretically possible
*P*: portion of the program that can be parallelized

*N*: number of available processors

Then the optimal work layout is defined as:
\begin{equation}
(1-P)+P/N
\end{equation}

And theoretically possible speedup is:
\begin{equation}
\frac{1}{(1-P) + \frac{P}{N}}
\end{equation}

*** WHNF/NF Evaluation
 Evaluate *a* to WHNF, then return *b*
#+BEGIN_SRC
seq a b :: a -> b -> b
#+END_SRC
 Evaluate *a* to WHNF in IO:
#+BEGIN_SRC
evaluate :: a -> IO a
#+END_SRC

Let's introduce a special class type:
#+BEGIN_SRC
class NFData where
   rnf :: a -> ()
   rnf a = a `seq` ()
#+END_SRC
It defaults to =seq= behavior which is fine for structures like =Bool=:
#+BEGIN_SRC
instance NFData Bool   ;;and many others in Control.Deepseq
#+END_SRC
Here's how to define instances for more complex datatypes:
#+BEGIN_SRC
instance NFData a => NFData (Tree a) where
   rnf Empty = ()
   rnf (Branch l a r) = rnf l `seq` rnf a `seq` rnf r
#+END_SRC
Higher level functions based on =NFData=:
#+BEGIN_SRC
deepseq :: NFData a => a -> b -> b   ;;like seq but for NF, not WHNF
deepseq a b = rnf a `seq` b

force :: NFData => a -> a
force x  = x `deepseq` x
#+END_SRC

We can see that evaluation to varying degrees is possible:
 - *WHNF*, O(1), weak evaluation
 - *NF*, O(n), deep evaluation (traverses the whole structure)

{{{PB}}}
*** Evaluation Strategies
Let's define the following type:
#+BEGIN_SRC
type Strategy a = a -> Eval a
#+END_SRC

Now we can speculate that =rpar= and =rseq= are also strategies:
#+BEGIN_SRC
rpar :: Strategy a
rseq :: Strategy a
#+END_SRC

We can introduce a little helper function:
#+BEGIN_SRC
using :: a -> Strategy a -> a
x `using` s = runEval (s x)
#+END_SRC

Now we can define higher level strategies:
#+BEGIN_SRC
parPair :: Strategy (a, b)
parPair (a, b) = do
   a' <- rpar a
   b' <- rpar b
   return (a', b')
#+END_SRC

We can use =parPair= stratey:
#+BEGIN_SRC
runEval (runPair(fib 35, fib 36))
#+END_SRC

Or if we rewrite with =using=:
#+BEGIN_SRC
(fib 35, fib 36) `using` parPair
#+END_SRC

*** Parameterized Strategies
We can define functions that build new strategies using existing ones.

First, let's make strategy =evalPair= for pair evaluation that is customizable by separate strategies for its components:
#+BEGIN_SRC
evalPair :: Strategy a -> Strategy b -> Strategy (a, b)
evalPair sa sb (a, b) = do
   a' <- sa a
   b' <- sb b
   return (a', b')
#+END_SRC
Second, let's define strategy =parPair= for parallel pair evaluation that is customizable by separate strategies for its components.
But first let's look at =rparWith= strategy that runs evaluation with supplied strategy but in parallel:
#+BEGIN_SRC
rparWith :: Strategy :: Strategy a -> Strategy a
rparWith strat = parEval . strat
#+END_SRC
Now let's use =rparWith= and =evalPair= to define =parPair=:
#+BEGIN_SRC
parPair :: Strategy a -> Straetgy b -> Strategy (a, b)
parPair sa sb = evalPair (rparWith sa) (rparWith sb)
#+END_SRC
Third, let's look at =rdeepseq= function:
#+BEGIN_SRC
rdeepseq :: NFData a => Strategy a
rdeepseq x = rseq (force x)
#+END_SRC
Now let's use it to build the final strategy:
#+BEGIN_SRC
parPair rdeepseq rdeepseq :: (NFData a, NFData b) => Strategy (a, b)
#+END_SRC
We build a strategy that deeply evaluates pair components in parallel by doing following steps:
 - Defined =evalPair=
 - =evalPair= with =rparWith= gave us =parPair=
 - =parPair= with =rdeepseq= gave us the final strategy
Let's use one more function, =r0=:
#+BEGIN_SRC
r0 :: Strategy a
r0 x = return x
#+END_SRC
Function =r0= avoids evaluation. Let's build a strategy that doesn't evaluate second components of a pair of pairs:
#+BEGIN_SRC
evalPair (evalPair rpar r0) (evalPair rpar r0) :: Strategy ((a, b), (a, b))
#+END_SRC

*** Speculative parallelism
Let's consider the following =evalList= implementations:
#+BEGIN_SRC
parList1 :: Strategy a -> Strategy [a]
parList1 stat = evalList (rparWith strat)

evalList :: Strategy a -> Strategy [a]
evalList strat [] = return []
evalList strat (x:xs) = do
   x' <- strat x
   xs <- evalList strat xs
   return (x':xs')

parList2 :: Strategy a -> Strategy [a]
parList2 strat xs = do
   go xs
   return xs
where
   go []     = return ()
   go (x:xs) = do rparWith strat x
                  go xs
#+END_SRC

Improtant thing to note is that in =parList1= we're building a new list. It might look like we might just generate sparks that would evaluate list items in parallel, as we did in =parList2=.
But this assumption is not true: if we only generate sparks, then only pool will reference them and they would be GC'd, i.e. we'd witness *speculative parallelism*.
Since we don't want it in this particular case, we need to have something else to reference sparks, hence building the resulting list: =parList1= is the correct implementation, =parList2= is not.

To summarize, a *bad use* of strategy looks like:
#+BEGIN_SRC
do
   ...
   rpar (f x)
   ...
#+END_SRC
*Good use* of stragy looks like:
#+BEGIN_SRC
do
   ...
   y <- rpar (f x)
   ... y ...
#+END_SRC
Or like this, if *y* is used somewhere in the program:
#+BEGIN_SRC
do
   ...
   rpar y
   ...
#+END_SRC

*** Practical considerations of dataflow parallelism
 - *Spark generation* will be likely done on different cores (switching cores)
 - *Granularity*
  - Generate enough work to make CPUs busy
  - But not too much
   - Overhead per chunk: creating, running
   - Amount of sequential work increases: need to merge results

*** Higher level functions
=parBuffer= creates sparks only for *N* first elements of list and keeps the number of sparks equal to *N* when some are evaluated:
#+BEGIN_SRC
parBuffer :: Int -> Strategy a -> Strategy [a]
#+END_SRC

=parListChunk= creates chunks of *N* elements each:
#+BEGIN_SRC
parListChunk :: Int -> Strategy a -> Strategy [a]
#+END_SRC

=withStrategy= is an alias for =using=:
#+BEGIN_SRC
withStrategy s x == x `using` s
#+END_SRC

*** The Identity Property
The value strategy returns must be equal to the value it was passed:
#+BEGIN_SRC
x `using` s == x
#+END_SRC
But there's a *caveat* though: =x `using` s= might be less defined than x, because it might evaluate more structure of x.
For example, compare how following expressions would be evaluated:
#+BEGIN_SRC
print $ snd (1 `div` 0, "Hello!")
print $ snd ((1 `div` 0, "Hello!") `using` rdeepseq)
#+END_SRC

{{{PB}}}

** Dataflow Pararllelism: The Par Monad

*Eval* monad allows expressing *data parallelism* which is parallelism between stream elements.

*Par* monad allows expressing:
 - *dataflow parallelism* which means declaratively defining a *dataflow network* with both independent (parallel) and dependent (graph edges) computations.
   - *pipeline parallelism*, i.e. parallelism between stages of a pipeline.
 - *data parallelism* (as in *Eval* monad chapter) as will be shown when we'll define =parMap= and =parMapM=.

There are also other important differences between the *Eval* and *Par* monad.

The *Eval* monad *pros*:
 - Decouples parallelism from algorithm
 - Composable evaluation strategies are possible
 - Can have as much parallelism as possible
The *Eval* monad *cons*:
 - We might not always want to build a lazy data structure
 - Might be tricky to doagnose and understand performance
The *Par* monad *pros*:
 - Explicit about granularity and data dependencies
 - Possible to avoid reliance on lazy evaluation but without sacrificing the determinism
The *Par* monad *cons*:
 - Only as much parallelism as there are pipelines
 - Only full structure evaluation is possible

*** Par Monad
Par monad related definitions:
#+BEGIN_SRC
newtype Par a
instance Applicative Par
instance Monad Par
runPar :: Par a -> a
fork :: Par () -> Par ()
#+END_SRC

IVar related definitions:
#+BEGIN_SRC
data IVar a
new  :: Par (IVar a)
put  :: NFData a => IVar a -> a -> Par ()  ;;strict, runs rdeepseq on a, hence NFData restriction
put_ :: IVar a -> a -> Par ()              ;;evaluates a to WHNF
get  :: IVar a -> Par a                    ;;blocking operation
#+END_SRC

Few important notes:
 - =put= operation is strict because if we request a parallel operation inside =Par= monad then it makes sense to make full evaluation a default behavior
 - =put_= is for when you want WNHF instead of NF, which is not a primary case by design
 - =get= operation will block
 - =IVar= instances are intended to be used in the =Par= monad where they were created. Breaking this rule might lead to deadlocks, runtime errors or other bad things.

*** Dataflow Parallelism
Together, =fork= and =IVar= allow the construction of *dataflow networks*. An example:
#+BEGIN_SRC
runPar $ do
   i <- new
   j <- new
   fork $ put i (fib n)
   fork $ put j (fib m)
   a <- get i
   b <- get j
   return $ a + b
#+END_SRC

We've created a *dataflow graph*:
- Each =fork= creates a node
- Each =new= creates and edge
- =get= and =put= connect the edges of nodes

*** Expressing data parallelism with Par Monad
Let's first define =spawn= to run computations in parallel and then =parMapM=.
#+BEGIN_SRC
spawn :: NFData a => Par a -> Par (IVar a)
spawn p = do
   i <- new
   fork (do x <-p; put i x)
   return i

parMapM :: NFData b => (a -> Par b) -> [a] -> Par [b]
parMapM f as = do
   ibs <- mapM (spawn . f) as
   mapM get ibs
#+END_SRC

Note that given following signatures:
#+BEGIN_SRC
mapM :: Monad m => (a -> m b) -> [a] -> m [b]
get  :: IVar a -> Par a
#+END_SRC
We can derive that:
#+BEGIN_SRC
spawn . f           :: a -> Par (IVar b)
mapM (spawn . f) as :: Par [IVar b]
ibs                 :: [IVar b]
mapM get ibs        :: Par [b]
#+END_SRC

Note that =parMapM= we defined here uses function that returns =Par=, meaning that *f* itself can add parallelism.
Now let's implement =parMap= that takes a non-monadic function *f* instead:
#+BEGIN_SRC
parMap :: NFData a => (a -> b) -> [a] -> Par [b]
parMap f as = do
   ibs <- mapM (spawn . return . f) as
   mapM get ibs
#+END_SRC
The only implementation difference from =parMapM= is =return= in =spawn . return . f= superposition because *f* now returns *b*, not *Par b*.

Both =parMapM= and =parMap= block and wait for results to compute because there's =get= operation in =mapM=. We can define a non-blocking version as
#+BEGIN_SRC
parMap2 :: NFData a => (a -> b) -> [a] -> Par [IVar b]
parMap2 f as = mapM (spawn . f) as
#+END_SRC

*** Pipeline Parallelism & Rate-Limiting the Producer
*Pipeline parallelism* means
- defining several stages that together form a pipeline
- each stage defines a single operation applied to all stream elements
- stages of pipeline work in parallel (which means that the amount of parallelism is limited)

Let's define *IList* and *Stream* types together with *streamFromList*, *streamFold*, *streamMap* functions:
#+BEGIN_SRC
data IList a
   = Nil
   | Cons a (IVar (IList a))

type Stream a = IVar (IList a)

streamFromList :: NFData a => [a] -> Par (Stream a)
streamFromList xs = do
   var <- new
   fork $ loop xs var
   return var
where
   loop [] var = put var Nil
   loop (x:xs) var = do
      tail <- new
      put var (Cons x tail)
      loop xs tail

streamFold :: (a -> b -> a) -> a -> Stream b -> Par a
streamFold fn !acc instrm = do
   ilist <- get instrim
   case ilist of
      Nil      -> return acc
      Cons h t -> streamFold fn (fn acc h) t

streamMap :: NFData b => (a -> b) -> Stream a -> Par (Stream b)
streamMap fn instream = do
   outstream <- new
   fork $ loop instream outstream
   return outstream
where
   loop instream outstream = do
      ilsit <- get instream
      case ilist of
         Nil -> put outstream Nil
         Cons h t -> do
            newtail <- new
            put outstream $ Cons (fn h) newtail
            loop t newtail
#+END_SRC

Now let's define 2 stages of a pipeline, =encrypt= and =decrypt=:
#+BEGIN_SRC
encrypt :: Integer -> Integer -> Strean ByteString -> Par (Stream ByteString)
encrypt n e s = streamMap (B.pack . show . power e n . code) s

decrypt :: Integer -> Integer -> Strean ByteString -> Par (Stream ByteString)
decrypt n d s = streamMap (B.pack . decode . power d n . integer) s
#+END_SRC

Then the pipeline will look like this:
#+BEGIN_SRC
pipeline :: Integer -> Integer -> Integer -> ByteString -> ByteString
pipeline n e d b = runPar $ do
   s0 <- streamFromList (chunk (size n) b)
   s1 <- encrypt n e s0
   s2 <- decrypt n d s1
   xs <- streamFold (\x y -> (y:x)) [] s2
   return (B.unlines (reverse xs))
#+END_SRC

Calls to =fork= in 2 =streamMap=, one =streamFromList=, and one =streamFold= calls make stages of pipeline work in parallel.

Two possible problems:
- if *consumer if much faster than producer* then we might not get enough parallelism
- but if *producer is much faster than consumer* then the producer might consume a lot of memory

The second problem can be solved by rate-limiting the producer. The general design is as follows:
- generate some amount of elements *2N*, but delay evaluation after *2N* elements
- put a rererence to *Par* (that continues evaluation elements of stream after *2N*) after element *N*
- when consumer consumes *N* elements, it finds saved reference to *Par* and then
 - consumer triggers evaluation of elements after *2N*
 - continues consuming second part of already existing *2N* elements while the next *2N* it triggered are being evaluated
 - the next *2N* elements are evaluated in the same way again, i.e. *N* elements are generated, reference to *Par* is saved, and more *N* elements are generated

The possible type definition:
#+BEGIN_SRC
data IList a
   = Nil
   | Cons a (IVar (IList a))
   | Fork (Par ()) (IList a)
#+END_SRC

*** Using Different Schedulers
The =Par= monad is implemented as a library, so its behavior can be changed without changing the compilier or runtime system.

For example, it's possible to use a different scheduling strategy. Make this change in all the modules that import =Control.Monad.Par=:

#+BEGIN_SRC
improt Control.Monad.Par.Scheds.Trace
   -- instead of Control.Monad.Par
#+END_SRC

** Comparison of Eval and Par Monads
|                           | Eval monad with strategies         | Par monad                                   |
|---------------------------+------------------------------------+---------------------------------------------|
| Laziness                  | Produces a lazy data structure     | Avoids reliance on lazy evaluation          |
| Separation from algorithm | Yes                                | No                                          |
| Composability             | Yes (strategies)                   | Only building a parallel skeleton           |
| Implementation            | Part of GHC                        | A library, can choose scheduling algorithms |
| Speculative parallelism   | Yes                                | No (sparks are always executed)             |
| ThreadScope integration   | Yes                                | No                                          |
| Overhead                  | Lower, better at low granularities | Higher                                      |
| Run method                | Almost free =runEval=              | Expensive (avoid nested =runPar= calls)     |

{{{PB}}}

* Concurrent Haskell

** Threads and MVars

*** Threads
#+BEGIN_SRC
forkIO :: IO () -> IO ThreadId
#+END_SRC

Threads may contend for a resource (stdout Handle for instance),
so their behavior is affected by how contention is managed by RTS.

The program terminaes when =main= returns even if there are threads running.
This is the simples possible solution,
since waiting for all threads can be implemented on top of it.

*** Mvars
#+BEGIN_SRC
data MVar a

newEmptyMVar :: IO (MVar a)
newMVar      :: a -> IO (MVar a)
takeMVar     :: MVar a -> IO a
putMVar      :: MVar a -> IO ()
#+END_SRC

=takeMVar= blocks if =MVar= is empty.
=putMVar= blocks if =MVar= is not empty.

=MVar= roles:
- *One place channel*: passing messages between threads, holding one message a time.
- *Container for shared mutable state*
 - Mutable data can be stored in =MVar=
 - Taking =MVar= is *acquiring the lock*
 - Putting the value with =putMVar= is *releasing the lock*
 - For external resources (foreign code, filesystem, etc.) dummy value () in =MVar= may be used to turn =putMVar= and =takeMVar= into operations for controlling access to these resources
- *Building block* for constructing larger data structures

*** MVar as a simple channel: A Logging Service
#+BEGIN_SRC
data Logger = Logger (MVar Log Channel)

;;MVar in Stop: call takeMVar on it to block
;;until Logging service allows to unblock with putMVar
data LogCommand = Message String 
                | Stop (MVar ())

initLogger :: IO Logger
initLogger = do
   m < newEmptyVar
   let l = Logger m
   forkIO (logger l)

logger :: Logger -> IO ()
logger (Logger m) = loop
   where
      loop = do
         cmd <- takeMVar m
         case cmd of
            Message msg -> do
               putStrLn msg
               loop
            Stop s -> do
               putStrLn "logger: stop"
               putMVar s ()

logMessage :: Logger -> String -> IO()
logMessage (Loger m) s = putMVar s (Message s)

logStop :: Logger -> IO ()
logStop (Logger m) = do
   s <- newEmptyMVar
   putMVar m (Stop s)
   takeMvar s
#+END_SRC

*** MVar as a container for shared state
#+BEGIN_SRC
type Name = String
type PhoneNumber = String
type PhoneBook = Map Name PhoneNumber
newtype PhoneBookState = PhoneBookState (MVar PhoneBook)

new :: IO PhoneBookState
new = do
   m <- newMVar Map.empty
   return $ PhoneBookState m

insert :: PhoneBookState -> Name -> PhoneNumber -> IO ()
insert (PhoneBookState m) name number = do
   book <- takeMVar m
   putMVar m $ Map.insert name number book   ;; (2)

lookup :: PhoneBookState -> Name -> IO (Maybe PhoneNumber)
lookup (PhoneBookState m) name = do
   book <- takeMVar m
   putMVar m book                 ;; (1)
   return $ Map.lookup name book
#+END_SRC

Important notes:
- In *(1)*, complex lookup operation happens outside of take/put lock: the lock isn't held too long.
- In *(2)*, =Map.insert= is lazy, this has 2 consequences:
 - Lock isn't being held for too long
 - But changes may build up (unevaluated)
There are two possible ways we can fix problem with *(2)*. First, we can use =$!= operator.
#+BEGIN_SRC
putMVar m $! Map.insert name number book
#+END_SRC
This reverses properties of the original solution:
- Lock is held until operation completes (too long)
- Changes (unevaled) can't build up
The second approach is to use =seq= to force evaluation of new *book state* after updating *MVar*:
#+BEGIN_SRC
let book' = Map.insert name number book
putMVar m book'
seq book' (return ())
#+ENd_SRC
Now the lock is held briefly, unevaluated changes don't build up.
*** MVar as a Building Block: Unbounded Channels
**** Implementing an unbounded channel

#+BEGIN_SRC
data Chan a = Chan (MVar (Stream a)) (MVar (Stream a))
data Stream a = MVar (Item a)
data Item a = Item a (Stream a)

#+END_SRC

#+BEGIN_SRC
+-------+
| h | t | Channel
+-------+
  |    |
+---+ +---+
| S | | S |---------------------------------------
+---+ +---+                                       |
  |                                               |
+--+  +------+  +--+  +------+  +--+  +------+  +--+
|  |->| Item |->|  |->| Item |->|  |->| Item |->|  |
+--+  +------+  +--+  +------+  +--+  +------+  +--+
Read  1st             2nd             3rd       Write
end   value           value           value     end

newChan :: IO (Chan a)
newChan = do
   hole     <- newEmptyVar
   readVar  <- newMVar hole
   writeVar <- newMVar hole
   return $ Chan readVar writeVar

writeChan :: Chane a -> a -> IO ()
writeChan (Chan _ writeVar) val = do
   newHole <- newEmptyVar
   oldHole <- takeMVar writeVar         ;; (1)
   putMVar oldHole $ Item val newHole   ;; (2)
   putMVar writeVar newHole

readChan :: Chan a -> IO a
readChan (Chan readVar _) = do
   stream <- takeMVar readVar           ;; (3)
   Item val tail <- takeMVar stream     ;; (4)
   putMVar readVar tail
   return val
#+END_SRC

*Several rules:*
- From =(1)= and =(3)= follows that all *writes* are independent from *reads*, if the channel is not empty.
- If the channel is empty, then *read* will block on =takeMVar= in =(4)= until a *write* occurs and unblocks =MVar= on =(2)=
- Every *write* blocks every other *write* (see =(1)=), every *read* blocks every other *read* (see =(3)=)

**** Implementing dupChannel

Let's try to implement =dupChannel= that allows to write to one channel, but read written value from both channels.
The idea: single common write end, but separate read ends.
The result of =dupChan= is a new channel that:
- uses the same =MVar= to track the last written value, so that both channel write to the end of the same stream
- but has a different =MVar= to track the read end, so that if a value is read from one channel, it still can be read from another

#+BEGIN_SRC
dupChan :: Chan a -> IO (Chan a)
dupChan (Chan _ writeVar) = do
   hole       <- readMVar writeVar
   newReadVar <- newMVar hole
   return     $  Chan newReadVar writeVar
#+END_SRC
*There's a problem* with this implementation:
in =(4)=, =readChan= takes value from =MVar= but doesn't put it back, thus destroying a link to the first =Item= from stream's =MVar=.
This means that when a value is read from duplicate channel, the reading thread will block on the empty =MVar=.

For example, here *read end 1* of one channel was read and it now points to *M2 MVar*, and the link from *M1* to *V1* was destroyed by =takeMVar=.
Now if a thread tries to read a value from another channel via *read end 2*, it will block indefinitely.
#+BEGIN_SRC
+-------+
| read  |
| end 1 |-------------------------
+-------+                         |
                                  |
+-------+                         |
| read  |    +----+    +----+    +----+    +----+    +----+
| end 2 |--->| M1 |-X->| V1 |--->| M2 |--->| V2 |--->| M3 |---> ...
+-------+    +----+    +----+    +----+    +----+    +----+
#+END_SRC

*The problem can be fixed* by replacing =takeMVar= with =readMVar= in =readChan= in =(4)=:
#+BEGIN_SRC
readChan :: Chan a -> IO a
readChan (Chan readVar _) = do
   stream <- takeMVar readVar
   Item val tail <- readMVar stream
   putMVar readVar tail
   return val
#+END_SRC

**** Implementing unGetChan

*Let's try to implement unGetChan*. The idea is to write to the read end (what could possibly go wrong?).
#+BEGIN_SRC
unGetChan :: Chan a -> a -> IO ()
unGetChan (Chan readVar _) val = do
   newReadEnd <- newEmptyMVar
   readEnd <- takeMVar readVar            ;; (5)
   putMVar newReadEnd (Item val readEnd)
   putMVar readVar newReadEnd
#+END_SRC

*There's a problem with this implementation*. If:
- the channel is empty
- and there's a =readChan= blocked on =takeMVar= because it's empty (see =(3)=)
then =unGetChan= will block on =takeMVar= on the same =MVar= as =readChan= (see =(5)=), and the value won't be written.

In other words, absence of value will block =readChan=,
but the following =unGetChan= won't be able to deliver the value and unblock =readChan= since it's blocked by the =readChan= itself. Deadlock.

*** Fairness

- If =MVar= is held 100% of the time then we can't guarantee anything
- But if it's not held indefinitely, then:
 - thread in the list of blocked threads will *wake up* & *acquire* =MVar= with =takeMVar= atomically, i.e. *no thread is blocked indefinitely* aka *fairness guarantee*
 - only one of the threads becomes unblocked, aka *single wakeup* proeprty that is an important performance characteristic when a large amount of threads are contending for a single =MVar=

These guarantees, *fairness guarantee* & *single wakeup* keep =MVar= from being completely subsumed by =STM=.

{{{PB}}}

** Synchronous Exceptions: Overlapping Input/Output

*** Async data type
Let's define =Async= data type:
#+BEGIN_SRC
data Async a = Async (MVar a)

async :: IO a -> IO (Async a)
async action = do
   var <- newEmptyMVar
   forkIO $ do r <- action; putMVar var r
   return $ Async var

wait :: Async a -> IO a
wait (Async var) = readMVar var
#+END_SRC
Using =readMVar=, not =takeMVar=, because there might be multiple waiters of this =Async=.

Example:
#+BEGIN_SRC
getURL :: String -> IO ByteString
getURL url = ...

timeDownload :: String -> IO ()
timeDownload url = do
   (page, time) <- timeit $ getURL url
   print "downloaded..."

sites = ["http://google.com", ...]

main = do
   as <- mapM (async . timeDownload) sites
   mapM_ wait as
#+END_SRC

*** Exceptions in Haskell

**** Definition

#+BEGIN_SRC
class (Typeable e, Show e) => Exception e where ...
#+END_SRC

**** Throwing

***** From anywhere

=throw= return an unrestricted type variable =a=:
#+BEGIN_SRC
throw :: Exception e => e -> a
#+END_SRC

Or:

#+BEGIN_SRC
error :: String -> a
error s = throw (ErrorCall s)
#+END_SRC

***** From IO

#+BEGIN_SRC
throwIO :: Exception e => a -> IO a
#+END_SRC

**** Catching

***** Catching and transforming

#+BEGIN_SRC
try :: Exception e => IO a -> IO (Either e a)
#+END_SRC

***** Catching and resource management
****** =onException=

Do the first action. If exception, do the second & rethrow exception
#+BEGIN_SRC
onException :: IO a -> IO b -> IO a
#+END_SRC

****** =bracket=

Defeines how to acquire resource (=before= action), what to do with it (=during= action), and how to release it (=after= action).

Release of resource is performed if the resource was acquired, whether the =during= action completed normally or abnormally via exception.

If =during= action completed with an exception, then the exception is rethrown.
#+BEGIN_SRC
bracket :: IO a -> (a -> IO b) -> (a -> IO c) -> IO c
bracket before after during = do
   a <- before
   c <- during a `onException` after a
   after a
   return c
#+END_SRC

****** =finally=

Perform the =io= action. Then perform the =after= action whether the =io= action completed normally or via exception.

If =io= action generated an exception, then the exception is rethrown.

#+BEGIN_SRC
finally :: IO a -> IO b -> IO a
finally io after = do
   io `onException` after
   after
#+END_SRC

*** Error Handling in Async

**** Redefining =Async= to include error information

#+BEGIN_SRC
data Async a = Async (MVar (Either SomeException a))

async action = do
   var <- newEmptyVar
   forkIO $ do r <- try action; putMVar var r
   return $ Async var
#+END_SRC

**** =waitCatch=
#+BEGIN_SRC
waitCatch :: Async a -> IO (Either SomeException a)
waitCatch (Async a) = readMVar a
#+END_SRC

**** =wait=
#+BEGIN_SRC
wait :: Async a -> IO a
wait a = do
   r <- waitCatch a
   case r of
      Left e -> throwIO e
      Right a -> return a
#+END_SRC

**** =waitEither=
#+BEGIN_SRC
waitEither :: Async a -> Async b -> IO (Either a b)
waitEither a b = do
   m <- newEmptyMVar
   forkIO $ do r <- try (fmap Left  (wait a)); putMVar m r
   forkIO $ do r <- try (fmap Right (wait b)); putMVar m r
   wait (Async m)
#+END_SRC

**** =waitAny=

#+BEGIN_SRC
waitAny :: [Async a] -> IO a
waitAny as = do
   m <- newEmptyMVar
   let forkwait a = forkIO $ do r <- try (wait a); putMVar m r
   mapM_ forkwait as
   wait (Async m)
#+END_SRC

{{{PB}}}
** Asynchronous Exceptions: Cancellation and Timeouts

*** Motivation

It's important to have an ability to interrupt the execution of another thread
after the occurence of some particular condition.

There are 2 ways to implement it:
- the thread has *to poll* for the cancellation condition
 - the programmer may forget to poll (or to poll regularly enough), and the thread will become unresponsive
- the thread is *immediately cancelled* in some way
 - critical sections that modify state need to be protected from cancellation (via first option, *polling*), otherwize cancellation may leave data in an unconsistent state

In most imperative languages, first option is the default behaviour because a lot of code modifies state.

In Haskell:
- most of the code is *purely functional*, so it *can be safely aborted/suspended/resumed* without affecting correctness, so interruption of another thread is performed via asynchronous exceptions by default
- most code is *purely functional*, so we *can't actually poll* for the cancellation timeouts

This is why Haskell uses the second approach
- threads are immediately cancelled by default
- but you can fall back to polling for critical sections

To summarise, there are 2 types of exceptions in Haskell:
- an exception from a function like =openFile= when the file doesn't exist --- *synchronous exceptions* (synchronous from the point of view of the victim thread)
- an exception that may arise an any time (because the user pressed the stop button) --- *asynchronous exceptions* (synchronous from the point of view of the victim thread)

*** Throwing
#+BEGIN_SRC
throwTo :: Exception e => ThreadId -> e -> IO ()
#+END_SRC

An example:
#+BEGIN_SRC
data Async a = Async ThreadId (MVar (Either SomeException a))

cancel :: Async a -> IO ()
cancel (Async t var) = throwTo t ThreadKilled

async :: IO () -> IO (Async a)
async action = do
   m <- newEmptyMVar
   t <- forkIO $ do r <- try action; putMVar m r
   return $ Async t m

main = do
   as <- mapM (async . timeDownload) sites
   forkIO $ do
      hSetBuffering stdin NoBuffering
      forever $ do
         c <- getChar
         when (c == 'q') $ mapM_ cancel as
   rs <- mapM waitCatch as
   printf "%d/%d succeeded\n" (length (rights rs)) (length rs)
#+END_SRC


*** Catching, Masking, and Unmasking

General problem: an asynchronous exception might be delivered in the middle of operation of updating shared state.
Here's an example:
#+BEGIN_SRC
problem :: MVar a -> (a -> IO a) -> IO ()
problem m f = do
   a <- takeMVar m                                 ;; (1)
   r < f a `catch` \e -> do putMVar m a; throw e   ;; (2)
   putMVar m r                                     ;; (3)
#+END_SRC

Locating the problem in this example:
- if =(1)= succeeds, and an async exception is thrown between =(1)= and =(2)=
- if =(1)= and =(2)= succeed, and an async exception is thrown between =(2)= and =(3)=
then the async exception will prevent =putMVar= in =(3)= from updating the shared =MVar=.

Let's solve this problem incrementally.

**** *Masking* (disabling) asynchronous exceptions for a region of code

Function =mask= lets us mask (disable, or switch to polling to be precise) asynchronous exceptions.
#+BEGIN_SRC
mask :: ((IO a -> IO a) -> IO b) -> IO b
#+END_SRC

We pass it some function =f= of type =(IO a -> IO a) -> IO b= that will receive a function =restore= of type =IO a -> IO a= as its first argument to unmask the exceptions.

Updated =problem= implementation looks like this:
#+BEGIN_SRC
problem :: MVar a -> (a -> IO a) -> IO ()
problem m f = mask $ \restore -> do
   a <- takeMVar m
   r <- restore (f a) `catch` \e -> do putMVar m a; throw e
   putMVar m r
#+END_SRC

We solved a part of the problem: async exceptions can now be delivered only in =restore (f a)=.

**** Ensure *blocking operations still respond* to masked asynchronous exceptions

The next problem is that =takeMVar= in =(1)= might block indefinitely & be unresponsive to asynchronous exceptions.

Solution: all blocking operations, like =putMVar= and =takeMVar=, are *interruptible* in Haskell:
- inside =mask= we're in polling mode
- if an operation actually blocks
then an *interruptible* function will throw an asynchronous exception as a synchronous exception.

The problem is hence solved: =takeMVar= will not block indefinitely.

**** But *not all blocking operations should respond* to asynchronous exceptions inside =mask=

The next problem is that we don't really want the =putMVar= in =(4)= to respond to a possible asynchronous exception.
This would leave shared =MVar= empty which is not the intended behavior of this code.

This can be solved only by operating on every =MVar= in a consistent way: first call =takeMVar=, then update it with =putMVar=.
Since we always call =takeMVar= on the =MVar= first, the =MVar= is guaranteed to be empty when we call =putMVar=, which means the =putMVar= is guarantee to not block, and since it doesn't block it will never respond to asynchronous exceptions.

**** Summary

We introduced =mask= that switches execution to *polling mode*, and =restore= that switches the mode back.

Being in *polling mode* means:
- For *most of the code*, being inside =mask= and hence switching to *polling mode* means that an asynchronous exception won't interrupt the execution.
- For *functions that can block*, like =putMVar= and =takeMVar=, it means that *if they actually block* then *they will respond* to a *polled asynchronous exception* if one occurs and *will throw it as a synchronous exception*.

*** Higher Level Abstractions

There are several more functions useful for working with asynchronous exceptions

**** =uninterruptibleMask=

There's a special version of =mask= that allows to hide asynchronous exceptions even from interruptible functions.

- If you mask async exceptions with =uninteruptibleMask= then all blocking operations won't poll an asynchronous exception when one happens.
- Hence the use of it must be treated with utmost suspicious

#+BEGIN_SRC
uninterruptibleMask :: ((IO a -> IO a) -> IO b) -> IO b
#+END_SRC

**** =getMaskingState=

It's possible to get current masking state for debugging purposes:
#+BEGIN_SRC
data MaskingState = Unmasked
                   | MaskedInterruptible
                   | MaskedUninterruptible

getMaskingState :: MaskingState
#+END_SRC

**** =modifyMVar=, =modifyMVar_=

Two following functions are intended to insulate the programmer from the need to mask directly:

#+BEGIN_SRC
modifyMVar_ :: MVar a -> (a -> IO a) -> IO ()
modifyMVar  :: MVar a -> (a -> IO (a, b)) -> IO b
#+END_sRC

Example:
#+BEGIN_SRC
modifyTwo :: MVar a -> MVar b -> (a -> b -> IO (a, b)) -> IO ()
modifyTwo ma mb = do
   modifyMVar_ mb $ \b ->           ;; (1)
      modifyMVar ma $ \a -> f a b   ;; (2)
#+END_SRC

Read =(2)= first, then read =(1)= to verify that the implementation does what it intended to.

We always know that both =modifyMVar= and =modifyMVar_= either apply changes or rethrow an exception. Hence only two scenarios are possible:
- Either =f a b= throws an exception (sync or async) when evaluated inside =(2)=. Then first =(2)= and after it =(1)= will restore original values of =MVars=.
- Or =f a b= completes normally, in which case =ma= gets updated, then =mb= gets updated

**** =bracket=

#+BEGIN_SRC
bracket :: IO a -> (a -> IO b) -> (a -> IO c) -> IO c
bracket before after thing =
   mask $ \restore -> do
      a <- before
      r <- restore (thing a) `onException` after a
      _ <- after a
      return r
#+END_SRC

There are two important characteristics of this utility:
- If =before= returns then =after= is guaranteed to be executed in the future
- =before= is allowed to contain only one blocking operation
 - if there's only one blocking operation in =before=, and it becomes blocked, and an asynchronous exception occurs then no harm is done: nothing was acquired, nothing needs to be released, the exception is simply rethrown
 - if there are two blocking operations in =before=, the first one succeeds, the second one blocks and detects an asynchronous exceptions, then the exception is rethrown, and possible harm has been done: the result of first blocking operation can't be undone!

*** Asynchronous exceptions in exception handlers

Haskell *utomatically masks asynchronous exceptions in exception handlers*:
- we don't want a cleanup to be interrupted
- but it's easy to accidentally remain inside the implicit mask

*** forkIO and Masking

An example:
#+BEGIN_SRC
async :: IO a -> IO (Async a)
async action = do
   m <- newEMptyMVar
   t <- forkIO $ do r < try action; putMVar m r
   return $ Async t m
#+END_SRC

The problem: if =Async= is cancelled then an exception may strike:
- after =try= & before =putMVar=
- before =try=
- even before =mask= if we wrap =forkIO= contents with it

Solution: *=forkIO= inherits mask state*. Now we can write:
#+BEGIN_SRC
async :: IO a -> IO (Async a)
async action = do
   m <- newEmptyMVar
   t <- mask $ \restore ->
      forkIO $ do r <- try (restore action); putMVar m r
   return $ Async t m
#+END_SRC

Also, since it's a common pattern, it's called =forkFinally=:
#+BEGIN_SRC
forkFinally :: IO a -> (Either SomeException a -> IO ()) -> IO ThreadId
forkFinally action fun =
   mask $ \restore ->
      forkIO $ do r <- try (restore action); fun r
#+END_SRC

Now the original =async= can be rewritten as:
#+BEGIN_SRC
async :: IO a -> IO (Async a)
async action = do
   m <- newEmptyMVar
   t <- forkFinally action $ putMVar m
   return $ Async t m
#+END_SRC

{{{PB}}}

** Software Transactional Memory

*** STM type and operations

#+BEGIN_SRC
data STM a
instance Monad STM

data TVar a
newTVar   :: a -> STM (TVar a)
readTVar  :: TVar a -> STM a
writeTVar :: TVAr a -> a -> STM ()

retry  :: STM a
orElse :: STM a -> STM a -> STM a

throwSTM :: Exception e => e -> STM a
catchSTM :: Exception e => STM a -> (e -> STM a) -> STM a
#+END_SRC

Operations inside =STM= form a transaction. Operations in a transaction are:
- *Transparent* to the rest of the program. There is no need to order operations based on what might happen outside of your *STM*:
 - *No deadlocks* caused by different MVar acquire/release orders.
 - *No need to acquire all MVars* before modification to avoid mixed state.
- *Composable*. Operations can be composed in larger =STMs=. This is why =STM= operations are provided without =atomically= wrapper.
- *Atomical*. =STM= is a different type from =IO=:
 - =STM= tracks all effects & can roll them back.
 - Effects are supposed to be only on =TVars= --- no arbitrary side effects allowed.

*** Blocking

#+BEGIN_SRC
retry :: STM a
#+END_SRC

What =retry= does:
- abandon current transaction and revert all changes done to =TVars=
- try again when one of mentioned =TVars= changes (blocked until then)

Example

#+BEGIN_SRC
type UserFocus = TVar Desktop

getWindows :: Display -> UserFocus -> STM (Set Window)
getWindows disp focus = do
   desktop <- readTVar focus
   readTVar (disp ! desktop)

render :: Set Window -> IO ()
render = ...

renderThread :: Display -> UserFocus -> IO ()
renderThread disp focus = do
   wins <- atomically $ getWindows disp focus   ;; transaction (1)
   loop wins
   where
      loop wins = do
         redner wins
         next <- atomically $ do                ;; transaction (2)
            wins' <- getWindows disp focus
            if (wins == wins')
               then retry
               else return wins
         loop next
#+END_SRC

Here, *transaction (2)*:
- will block on =retry= if the set of windows hasn't changed
- will be unblocked and rerun every time the =TVar= (that represents user focus of the type =UserFocus= changes) expecting some thread changed the =TVar=
- will be blocked again if the set of windows hasn't changed yet

*** Merging

#+BEGIN_SRC
orElse :: STM a -> STM a -> STM a
#+END_SRC

What =orElse a b= does:
- returns =a= if =a= succeeds
- if =a= retries then =b= is executed

Note that =orElse= is left biased:
 - can have implications for fairness
 - must decide if it fits application requriements

There two ways to compose =STM= operations:
 - =>>== is "and"
 - =orElse= is "or"

Examples
#+BEGIN_SRC
takeEitherTMVar :: TMVar a -> TMVar b -> STM (Either a b)
takeEitherTMVar ma mb =
   fmap Left (takeTMvar ma)
      `orElse`
   fmap Right (takeTMvar mb)

waitAny :: [Async a] -> IO a
waitAny asyncs = atomically $ foldr orElse retry $ map waitSTM asyncs
#+END_SRC

*** Asynchronous Exceptions Safety

#+BEGIN_SRC
throwSTM :: Exception e => e -> STM a
catchSTM :: Exception e => STM a -> (e -> STM a) -> STM a
#+END_SRC

If there's *an exception*, either syncrhonous or asynchronous, during STM execution,
then *all effects are discarded.*

*** What Can't Be Done with STM: STM vs MVar

**** =MVar= is in general faster than =STM=

This doesn't mean the code using =MVar= is always faster than the code using =STM=.

**** =MVar= advantage is *fairness*

- Threads blocked on =MVar= will be:
 - *woken up in FIFO order*
 - *no single thread is blocked forever* when both:
  - there's supply of =putMVar= invocations
  - no thread holds =MVar= 100% of the time
- Threads blocked on =TVar=:
 - specific waited condition is not known ==>= have to *wake up all threads* for they all might (in general) make progress
 - *no fairness guarantee* since all threads are being woke up

Also, we can't even implement fairness using =STM=:

#+BEGIN_SRC
data TMVar a = TMVar (TVar (Maybe a))
                     (TVar [TVar (Maybe a)])
#+END_SRC

Here we have a =TVar= with a value and a list of =TVars= that represent operations.
Possible states:
- =TVar= is empty
 - list of operations is empty
 - list of operations contains =MVars= that are empty and were put by =takeMVar= operations; these operations are blocked on these =MVars= and wait for a value to be set
- =TVar= has a value
 - list of operations is empty
 - list of operations contains =MVars= with values put by =putMVar= operations blocked on these =MVars= waiting for them to be read

Implementation of =putMVar= would behave as follows:
- if =TVar= is empty and there are no blocked =takeMVar= operations: store the value and return
- if =TVar= is empty and there are some blocked =takeMVar= operations: remove the first blocked operations, supply it's =TVar= with the value and return
- if =TVar= is full: *put =TVar= containing =Just a= in the list* of waiting blocked operations and *block until this =TVar becomes empty* (someone reads it)

The last one is something we can't do using =STM=: we *can't both modify a list (perform an effect) and retry at the same time.*

In general, =STM= *can't express multiway communication between threads* if these operations need to:
- block
- have a visible effect advertising that they are blocked

*** Performance

There are several aspects of =STM= implementation to be aware of:

**** Log of operations

An =STM= transaction accumulates a log of =readTVar= & =writeTVar= operations
 - *discarding effects* is easy if you store operations instead of applying them ==>= *fixed small cost of transaction aborting*
 - *retrying is easy*: all =TVars= that might change can be found in the log
 - but *each =readTVar= has to traverse the log to check if the =TVar= was written by some other =TVar= in the transaction*

**** When transaction ends

=STM= implementation will compare the log against the contents of main memory:
- if =readTVar= *initially read different values*, then *the log is discarded and the transaction reruns* from the very beginning
- otherwise, *transaction is committed* to the memory
 - for the duration of the commit, all =TVars= involved in the transaction are locked
  - transactions operating on disjoint sets of =TVars= will proceed without interference

**** When transaction retries

- Every =TVar= has a *watch list of threads that should be woken up*
- =retry= operation finds all =TVars= accessed by the transaction and adds current thread to the watch list of all found =TVars=
- When transaction is committed *all threads in all =TVar= watch list are woken up*

**** Tips

- *Don't read unbounded number of =TVars= in a single transaction*
 - =readTVar= is =O(n)= ==>= whole transaction complexity is =O(n^2)= where =n= is the number of =TVars=.
- *Expensive evaluation inside transaction* increases the chance of reexecution (due to another transaction successful change of same =TVars=).
- Watch out *composing too many blocking operations*, as in =atomicatlly $ mapM takeMVar ts=
 - If =TVars= become available one at a time then the transaction will reexecute for each of them, so the complexity of transaction would be =O(n^2)=.

{{{PB}}}

** Higher Level Concurrency Abstractions

*** Symmetric Concurrency Combinators

**** =waitBoth=, =waitEither=, =waitAsync=

We've already defined =waitEither=, let's also define =waitBoth= and =waitAsync=:
#+BEGIN_SRC
waitEither :: Async a -> Async b -> IO (Either a b)

waitBoth :: Async -> Async b -> IO (a, b)
waitBoth a1 a2 = do
   atomically $ do
      r1 <- waitSTM a1 `orElse` (do waitSTM a2; retry)   ;; (1)
      r2 <- waitSTM a2
      return (r1, r2)

waitAsync :: IO a -> (Async -> IO b) -> IO b
waitAsync io operation = bracket (async io) cancel operation
#+END_SRC

Note the =orElse= in =(1)=:
- Both =r1= and =r2= can have 3 outcomes: *successful*, *retried*, *exception*.
- =waitBoth= must be symmetric, i.e. =waitBoth r1 r2= must have the same kind of outcome as =waitBoth r2 r1=.
 - the same kind of outcome, not the same outcome: if both =r1= and =r2= throw an exception, we can rethrow only one of them
- if we ignore *exception* outcome for a moment, it's obvious that implementation without =orElse= in =(1)= would be symmetric:
 - both *successful* results lead to *successful* result of =waitBoth=
 - a single *retried* in either =a1= or =a2= (or in both) means *retried* result of =waitBoth=.
- Now let's get back to *exception* outcome and put it back to possible outcomes:
 - *exception* and *successful* outcomes work well together without *orElse* in =(1)=: an *exception* for either of arguments means and *exception* for the whole =waitBoth=
 - but *exception* and *retry* outcomes of arguments don't play well together without =orElse= in =(1)=:
  - if =a1= throws, =a2= retries, then =waitBoth= throws
  - if =a1= retries, =a2= throws, then =waitBoth= retries --- i.e. it ignores the exception of =a2=, so we add =orElse= in =(1)= just to check for a possible exception thrown by =a2= to make =waitBoth= symmetrical

Note about =withAsync= implementation:
 if embedded code (operation arg) fails with exception then withAsync will cancel the thread & rethrow the exception ==>= no thread leackage.

**** =concurrently= and =race=

Let's use =waitBoth=, =waitEither=, and =waitAsync= defined above:

#+BEGIN_SRC
concurrently :: IO a -> IO b -> IO (a, b)
concurrently ioa iob =
   withAsync ioa $ \a ->      ;; (2)
      withAsync iob $ \b ->   ;; (2)
         waitBoth a b         ;; (3)

race :: IO a -> IO b -> IO (a, b)
race ioa iob =
   withAsync ioa $ \a ->      ;; (2)
      withAsync iob $ \b ->   ;; (2)
         waitEither a b       ;; (3)
#+END_SRC

Functions =concurrently= and =race= imply two rules for concurrent trees of computation that they allow to define:
- if a thread throws an exception then children threads will be cancelled (async ThreadKilled exception)
- if a child thread raised an exception it is propagated to the parent

In other words, exception at every level kills all children and propagates upwards, i.e. the whole tree is destroyed.

**** Timeout using =race=

#+BEGIN_SRC
timeout :: Int -> IO a -> IO (Maybe a)
timeout n m
   | n < 0     = fmap Just m
   | n == 0    = return Nothing
   | otherwise = do
         r <- race (threadDelay n) m
         case r of
            Left _  -> return Nothing
            Right a -> return (Just a)
#+END_SRC

*** Adding a Functor Instance

**** Motivation

In =waitAny= we might want to wait for several =Asyncs=, but they might have different types:
#+BEGIN_SRC
waitAny :: [Async] -> IO a
waitAny asyncs = atomically $ foldr orElse retry $ map waitSTM asyncs
#+END_SRC

**** Solution: add a Functor instance

#+BEGIN_SRC
class Functor f where 
   fmap :: (a -> b) -> f a -> f b
#+END_SRC

**** Problem with adding a Functor instance

We can't operate on =TVar= with =fmap=: it's effectful, must be inside =STM=
#+BEGIN_SRC
data Async a = Async ThreadId (TMVar (Either SomeException a))
#+END_SRC

**** Solution

Let's replace =TVar= with =STM=

#+BEGIN_SRC
data Async a = Async ThreadId (STM (Either SomeException a))

instance Functor Async where
   fmap f (Async t stm) = Async t stm'
      where stm' = do
         r <- stm
         case r of
            Left e -> return (Left e)
            Right a -> return (Right (f a))
#+END_SRC

{{{PB}}}

** Debugging, Tuning, and Interfacing with Foreign Code
