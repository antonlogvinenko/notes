#+Title: Parallel and Concurrent Programming in Haskell: Summary
#+Author: Anton Logvinenko
#+Email: anton.logvinenko@gmail.com
#+latex_header: \hypersetup{colorlinks=true,linkcolor=blue}
#+latex_header: \usepackage{parskip}
#+latex_header: \linespread{1}
#+MACRO: PB @@latex:\pagebreak@@ @@html: <br/><br/><br/><hr/><br/><br/><br/>@@ @@ascii: |||||@@
#+LATEX_HEADER: \usepackage[margin=0.75in]{geometry}

{{{PB}}}

* Intro
This is a summary of Simon Marlow's book [[https://simonmar.github.io/pages/pcph.html]["Parallel and Concurrent Prorgamming in Haskell"]].

Free online version is available [[https://www.oreilly.com/library/view/parallel-and-concurrent/9781449335939/][here]].


{{{PB}}}

* Concurrency and Parallelism
*Parallel program* uses a multiplicity of hardware to perform a computation more quickly: *to arrive at the answer earlier*.
Alternatives: better algorithm, lower quality, better hardware.
*Concurrency*: program-structuring technique in which there are *multiple threads of control* to
*interact with multiple independent external agents*.
Conceptually, threads run at the same time whether they actually execute at the same time is an implementation detail.
Alternatives: event loops, callbacks.

*Detetrministic* programming model: a program can give only one result. *Non-deterministic* programming model: a program may have many results.

*Concurrent* programming models are *necessarily non-deterministic*. They interact with external agents that cause events at unpredictable times.
*Most parallel* programming models *can be deterministic*. Exceptions:
 - some algorithms depend on internal non-determinism
 - when you want to parallelize programs that do have side effects

*In Haskell*, concurrency (in general) is a structuring technique for effectful code.
Pure code has no effects to observe & evaluation order doesn't matter.

{{{PB}}}

* Parallel Haskell
*Automatic parallelization problem*: to make program faster we need to gain more from parallelism than we lose due to overhead.
*Alternative*: use profiling to find candidates for parallelism. Parallel programs in Haskell *elimitate* some difficulties:
 - parallel programming is deterministic in Haskell
 - use of high-level & declarative models, without having to deal with synchronization and communication: programmer indicates where the parallelism is & the RTS will handle the details of running program in parallel
  - programs are abstract and likely to work on more hardware
  - takes advantage of RTS GC
  - but performance problems can be hard to understand

The main thing to think about in parallel Haskell is *partitioning*:
 - *granularity*: large enough to dwarf overhead, but not too large to keep all CPUs busy
 - *data dependencies*: when one task depends on another htye must be executed sequentially
   - *implicit data dependencies* are more concise, but it may be more difficult to reason about performance and fix problems
   - *explicit data dependencies* are less concise but easier to analyze

{{{PB}}}

** Data Parallelism: the Eval Monad
*** Lazy Evaluation, Weak Head Normal Form
Relevant commands:
 - =:sprint= prints value without causing it to be evaluated
 - =seq a b= evals a to WHNF, then returns *b*

General principles:
 - defining an expression causes a /thunk/ to be built
 - a thunk remains /unevaluated/ until the value is required
 - once evaluated, the thunk is /replaced/ by a value

An expression is in WHNF if it's either:
 - a constructor: =True=, =(:) 1=
 - lambda abstraction: =\x -> expression=
 - built-in function applied to too few arguments: =(+) 2=
Exception: fully applied constructor for a datatype with some fields declared strict.

How to test whether in WHNF:
 * =:sprint x=
 * =seq x ()=
 * =:sprint x=
 * If =:sprtin x= gives identical results then *x* was in WHNF

*** The Eval Monad, rpar, and rseq
#+BEGIN_SRC Haskell
data Eval a
instance Monad Eval

runEval :: Eval a -> a
rpar    :: a -> Eval a   ;;evaluate in parallel, don't wait
rseq    :: a -> Eval a   ;;evaluate & wait
#+END_SRC

Typical use
 if we expect to generate *more parallelism* soon or if we *don't depend on the result* of either operations
#+BEGIN_SRC Haskell
runEval $ do
   a <- rpar (f x)
   b <- rpar (f y)
   return (a, b)
#+END_SRC

Typical use if we *generated all the parallelism* we need or if we *depend on results* of operations:
#+BEGIN_SRC Haskell
runEval $ do
   a <- rpar (f x)
   b <- rpar (f y)
   rseq a
   rseq b
   return (a, b)
#+END_SRC

*** ThreadScope, compiler & executable options for parallelism
Compiler:
 - =eventlog= enable =-l= option for binary
 - =threaded= compile with parallelis,
 - =rtsoprts= enable +RTS option for binary
Executable:
 - =+RTS= starts passing RTS flags
 - =-RTS= closes sequemce of RTS flags (optional if nothing goes after them)
 - =+RTS -s= display statistics
 - =-RTS -l= generate log that can be opened with ThreadScope

*** GHC dynamic partitioning, Amdahl's Law
GHC sparks:
 - =rpar= argument is a *spark*
 - sparks are collected in a pool
 - sparks are taken from pool when processors are available
 - RTS uses work stealing to execute sparks
GHC spark can be in following states:
 - *converted* into real parallism
 - *overflowed* pool of limited size was overflowed, sparks dropped
 - *dud*: rpar was applied to already evaluated expression
 - *GC'd*: spark was found to be unused by the program
 - *fizzled*: unevaluated when pased to rpar, but evaluated later (dropped from pool)

Amdahl's Law explains how much parallelism is theoretically possible
*P*: portion of the program that can be parallelized

*N*: number of available processors

Then the optimal work layout is defined as:
\begin{equation}
(1-P)+P/N
\end{equation}

And theoretically possible speedup is:
\begin{equation}
\frac{1}{(1-P) + \frac{P}{N}}
\end{equation}

*** WHNF/NF Evaluation
 Evaluate *a* to WHNF, then return *b*
#+BEGIN_SRC
seq a b :: a -> b -> b
#+END_SRC
 Evaluate *a* to WHNF in IO:
#+BEGIN_SRC
evaluate :: a -> IO a
#+END_SRC

Let's introduce a special class type:
#+BEGIN_SRC
class NFData where
   rnf :: a -> ()
   rnf a = a `seq` ()
#+END_SRC
It defaults to =seq= behavior which is fine for structures like =Bool=:
#+BEGIN_SRC
instance NFData Bool   ;;and many others in Control.Deepseq
#+END_SRC
Here's how to define instances for more complex datatypes:
#+BEGIN_SRC
instance NFData a => NFData (Tree a) where
   rnf Empty = ()
   rnf (Branch l a r) = rnf l `seq` rnf a `seq` rnf r
#+END_SRC
Higher level functions based on =NFData=:
#+BEGIN_SRC
deepseq :: NFData a => a -> b -> b   ;;like seq but for NF, not WHNF
deepseq a b = rnf a `seq` b

force :: NFData => a -> a
force x  = x `deepseq` x
#+END_SRC

We can see that evaluation to varying degrees is possible:
 - *WHNF*, O(1), weak evaluation
 - *NF*, O(n), deep evaluation (traverses the whole structure)

{{{PB}}}
*** Evaluation Strategies
Let's define the following type:
#+BEGIN_SRC
type Strategy a = a -> Eval a
#+END_SRC

Now we can speculate that =rpar= and =rseq= are also strategies:
#+BEGIN_SRC
rpar :: Strategy a
rseq :: Strategy a
#+END_SRC

We can introduce a little helper function:
#+BEGIN_SRC
using :: a -> Strategy a -> a
x `using` s = runEval (s x)
#+END_SRC

Now we can define higher level strategies:
#+BEGIN_SRC
parPair :: Strategy (a, b)
parPair (a, b) = do
   a' <- rpar a
   b' <- rpar b
   return (a', b')
#+END_SRC

We can use =parPair= stratey:
#+BEGIN_SRC
runEval (runPair(fib 35, fib 36))
#+END_SRC

Or if we rewrite with =using=:
#+BEGIN_SRC
(fib 35, fib 36) `using` parPair
#+END_SRC

*** Parameterized Strategies
We can define functions that build new strategies using existing ones.

First, let's make strategy =evalPair= for pair evaluation that is customizable by separate strategies for its components:
#+BEGIN_SRC
evalPair :: Strategy a -> Strategy b -> Strategy (a, b)
evalPair sa sb (a, b) = do
   a' <- sa a
   b' <- sb b
   return (a', b')
#+END_SRC
Second, let's define strategy =parPair= for parallel pair evaluation that is customizable by separate strategies for its components.
But first let's look at =rparWith= strategy that runs evaluation with supplied strategy but in parallel:
#+BEGIN_SRC
rparWith :: Strategy :: Strategy a -> Strategy a
rparWith strat = parEval . strat
#+END_SRC
Now let's use =rparWith= and =evalPair= to define =parPair=:
#+BEGIN_SRC
parPair :: Strategy a -> Straetgy b -> Strategy (a, b)
parPair sa sb = evalPair (rparWith sa) (rparWith sb)
#+END_SRC
Third, let's look at =rdeepseq= function:
#+BEGIN_SRC
rdeepseq :: NFData a => Strategy a
rdeepseq x = rseq (force x)
#+END_SRC
Now let's use it to build the final strategy:
#+BEGIN_SRC
parPair rdeepseq rdeepseq :: (NFData a, NFData b) => Strategy (a, b)
#+END_SRC
We build a strategy that deeply evaluates pair components in parallel by doing following steps:
 - Defined =evalPair=
 - =evalPair= with =rparWith= gave us =parPair=
 - =parPair= with =rdeepseq= gave us the final strategy
Let's use one more function, =r0=:
#+BEGIN_SRC
r0 :: Strategy a
r0 x = return x
#+END_SRC
Function =r0= avoids evaluation. Let's build a strategy that doesn't evaluate second components of a pair of pairs:
#+BEGIN_SRC
evalPair (evalPair rpar r0) (evalPair rpar r0) :: Strategy ((a, b), (a, b))
#+END_SRC

*** Speculative parallelism
Let's consider the following =evalList= implementations:
#+BEGIN_SRC
parList1 :: Strategy a -> Strategy [a]
parList1 stat = evalList (rparWith strat)

evalList :: Strategy a -> Strategy [a]
evalList strat [] = return []
evalList strat (x:xs) = do
   x' <- strat x
   xs <- evalList strat xs
   return (x':xs')

parList2 :: Strategy a -> Strategy [a]
parList2 strat xs = do
   go xs
   return xs
where
   go []     = return ()
   go (x:xs) = do rparWith strat x
                  go xs
#+END_SRC

Impotant thing to note is that in =parList1= we're building a new list. It might look like we might just generate sparks that would evaluate list items in parallel, as we did in =parList2=.
But this assumption is not true: if we only generate sparks, then only pool will reference them and they would be GC'd, i.e. we'd witness *speculative parallelism*.
Since we don't want it in this particular case, we need to have something else to reference sparks, hence building the resulting list: =parList1= is the correct implementation, =parList2= is not.

To summarize, a *bad use* of strategy looks like:
#+BEGIN_SRC
do
   ...
   rpar (f x)
   ...
#+END_SRC
*Good use* of stragy looks like:
#+BEGIN_SRC
do
   ...
   y <- rpar (f x)
   ... y ...
#+END_SRC
Or like this, if *y* is used somewhere in the program:
#+BEGIN_SRC
do
   ...
   rpar y
   ...
#+END_SRC

*** Practical considerations of dataflow parallelism
 - *Spark generation* will be likely done on different cores (switching cores)
 - *Granularity*
  - Generate enough work to make CPUs busy
  - But not too much
   - Overhead per chunk: creating, running
   - Amount of sequential work increases: need to merge results

*** Higher level functions
=parBuffer= creates sparks only for *N* first elements of list and keeps the number of sparks equal to *N* when some are evaluated:
#+BEGIN_SRC
parBuffer :: Int -> Strategy a -> Strategy [a]
#+END_SRC

=parListChunk= creates chunks of *N* elements each:
#+BEGIN_SRC
parListChunk :: Int -> Strategy a -> Strategy [a]
#+END_SRC

=withStrategy= is an alias for =using=:
#+BEGIN_SRC
withStrategy s x == x `using` s
#+END_SRC

*** The Identity Property
The value strategy returns must be equal to the value it was passed:
#+BEGIN_SRC
x `using` s == x
#+END_SRC
But there's a *caveat* though: =x `using` s= might be less defined than x, because it might evaluate more structure of x.
For example, compare how following expressions would be evaluated:
#+BEGIN_SRC
print $ snd (1 `div` 0, "Hello!")
print $ snd ((1 `div` 0, "Hello!") `using` rdeepseq)
#+END_SRC

{{{PB}}}

** Dataflow Pararllelism: The Par Monad

*Eval* monad allows expressing *data parallelism* which is parallelism between stream elements.

*Par* monad allows expressing:
 - *dataflow parallelism* which means declaratively defining a *dataflow network* with both independent (parallel) and dependent (graph edges) computations.
   - *pipeline parallelism*, i.e. parallelism between stages of a pipeline.
 - *data parallelism* (as in *Eval* monad chapter) as will be shown when we'll define =parMap= and =parMapM=.

There are also other important differences between the *Eval* and *Par* monad.

The *Eval* monad *pros*:
 - Decouples parallelism from algorithm
 - Composable evaluation strategies are possible
 - Can have as much parallelism as possible
The *Eval* monad *cons*:
 - We might not always want to build a lazy data structure
 - Might be tricky to doagnose and understand performance
The *Par* monad *pros*:
 - Explicit about granularity and data dependencies
 - Possible to avoid reliance on lazy evaluation but without sacrificing the determinism
The *Par* monad *cons*:
 - Only as much parallelism as there are pipelines
 - Only full structure evaluation is possible

*** Par Monad
Par monad related definitions:
#+BEGIN_SRC
newtype Par a
instance Applicative Par
instance Monad Par
runPar :: Par a -> a
fork :: Par () -> Par ()
#+END_SRC

IVar related definitions:
#+BEGIN_SRC
data IVar a
new  :: Par (IVar a)
put  :: NFData a => IVar a -> a -> Par ()  ;;strict, runs rdeepseq on a, hence NFData restriction
put_ :: IVar a -> a -> Par ()              ;;evaluates a to WHNF
get  :: IVar a -> Par a                    ;;blocking operation
#+END_SRC

Few important notes:
 - =put= operation is strict because if we request a parallel operation inside =Par= monad then it makes sense to make full evaluation a default behavior
 - =put_= is for when you want WNHF instead of NF, which is not a primary case by design
 - =get= operation will block
 - =IVar= instances are intended to be used in the =Par= monad where they were created. Breaking this rule might lead to deadlocks, runtime errors or other bad things.

*** Dataflow Parallelism
Together, =fork= and =IVar= allow the construction of *dataflow networks*. An example:
#+BEGIN_SRC
runPar $ do
   i <- new
   j <- new
   fork $ put i (fib n)
   fork $ put j (fib m)
   a <- get i
   b <- get j
   return $ a + b
#+END_SRC

We've created a *dataflow graph*:
- Each =fork= creates a node
- Each =new= creates and edge
- =get= and =put= connect the edges of nodes

*** Expressing data parallelism with Par Monad
Let's first define =spawn= to run computations in parallel and then =parMapM=.
#+BEGIN_SRC
spawn :: NFData a => Par a -> Par (IVar a)
spawn p = do
   i <- new
   fork (do x <-p; put i x)
   return i

parMapM :: NFData b => (a -> Par b) -> [a] -> Par [b]
parMapM f as = do
   ibs <- mapM (spawn . f) as
   mapM get ibs
#+END_SRC

Note that given following signatures:
#+BEGIN_SRC
mapM :: Monad m => (a -> m b) -> [a] -> m [b]
get  :: IVar a -> Par a
#+END_SRC
We can derive that:
#+BEGIN_SRC
spawn . f           :: a -> Par (IVar b)
mapM (spawn . f) as :: Par [IVar b]
ibs                 :: [IVar b]
mapM get ibs        :: Par [b]
#+END_SRC

Note that =parMapM= we defined here uses function that returns =Par=, meaning that *f* itself can add parallelism.
Now let's implement =parMap= that takes a non-monadic function *f* instead:
#+BEGIN_SRC
parMap :: NFData a => (a -> b) -> [a] -> Par [b]
parMap f as = do
   ibs <- mapM (spawn . return . f) as
   mapM get ibs
#+END_SRC
The only implementation difference from =parMapM= is =return= in =spawn . return . f= superposition because *f* now returns *b*, not *Par b*.

Both =parMapM= and =parMap= block and wait for results to compute because there's =get= operation in =mapM=. We can define a non-blocking version as
#+BEGIN_SRC
parMap2 :: NFData a => (a -> b) -> [a] -> Par [IVar b]
parMap2 f as = mapM (spawn . f) as
#+END_SRC

*** Pipeline Parallelism & Rate-Limiting the Producer
*Pipeline parallelism* means
- defining several stages that together form a pipeline
- each stage defines a single operation applied to all stream elements
- stages of pipeline work in parallel (which means that the amount of parallelism is limited)

Let's define *IList* and *Stream* types together with *streamFromList*, *streamFold*, *streamMap* functions:
#+BEGIN_SRC
data IList a
   = Nil
   | Cons a (IVar (IList a))

type Stream a = IVar (IList a)

streamFromList :: NFData a => [a] -> Par (Stream a)
streamFromList xs = do
   var <- new
   fork $ loop xs var
   return var
where
   loop [] var = put var Nil
   loop (x:xs) var = do
      tail <- new
      put var (Cons x tail)
      loop xs tail

streamFold :: (a -> b -> a) -> a -> Stream b -> Par a
streamFold fn !acc instrm = do
   ilist <- get instrim
   case ilist of
      Nil      -> return acc
      Cons h t -> streamFold fn (fn acc h) t

streamMap :: NFData b => (a -> b) -> Stream a -> Par (Stream b)
streamMap fn instream = do
   outstream <- new
   fork $ loop instream outstream
   return outstream
where
   loop instream outstream = do
      ilsit <- get instream
      case ilist of
         Nil -> put outstream Nil
         Cons h t -> do
            newtail <- new
            put outstream $ Cons (fn h) newtail
            loop t newtail
#+END_SRC

Now let's define 2 stages of a pipeline, =encrypt= and =decrypt=:
#+BEGIN_SRC
encrypt :: Integer -> Integer -> Strean ByteString -> Par (Stream ByteString)
encrypt n e s = streamMap (B.pack . show . power e n . code) s

decrypt :: Integer -> Integer -> Strean ByteString -> Par (Stream ByteString)
decrypt n d s = streamMap (B.pack . decode . power d n . integer) s
#+END_SRC

Then the pipeline will look like this:
#+BEGIN_SRC
pipeline :: Integer -> Integer -> Integer -> ByteString -> ByteString
pipeline n e d b = runPar $ do
   s0 <- streamFromList (chunk (size n) b)
   s1 <- encrypt n e s0
   s2 <- decrypt n d s1
   xs <- streamFold (\x y -> (y:x)) [] s2
   return (B.unlines (reverse xs))
#+END_SRC

Calls to =fork= in 2 =streamMap=, one =streamFromList=, and one =streamFold= calls make stages of pipeline work in parallel.

Two possible problems:
- if *consumer if much faster than producer* then we might not get enough parallelism
- but if *producer is much faster than consumer* then the producer might consume a lot of memory

The second problem can be solved by rate-limiting the producer. The general design is as follows:
- generate some amount of elements *2N*, but delay evaluation after *2N* elements
- put a rererence to *Par* (that continues evaluation elements of stream after *2N*) after element *N*
- when consumer consumes *N* elements, it finds saved reference to *Par* and then
 - consumer triggers evaluation of elements after *2N*
 - continues consuming second part of already existing *2N* elements while the next *2N* it triggered are being evaluated
 - the next *2N* elements are evaluated in the same way again, i.e. *N* elements are generated, reference to *Par* is saved, and more *N* elements are generated

The possible type definition:
#+BEGIN_SRC
data IList a
   = Nil
   | Cons a (IVar (IList a))
   | Fork (Par ()) (IList a)
#+END_SRC

*** Using Different Schedulers
The =Par= monad is implemented as a library, so its behavior can be changed without changing the compilier or runtime system.

For example, it's possible to use a different scheduling strategy. Make this change in all the modules that import =Control.Monad.Par=:

#+BEGIN_SRC
improt Control.Monad.Par.Scheds.Trace
   -- instead of Control.Monad.Par
#+END_SRC

** Comparison of Eval and Par Monads
|                           | Eval monad with strategies         | Par monad                                   |
|---------------------------+------------------------------------+---------------------------------------------|
| Laziness                  | Produces a lazy data structure     | Avoids reliance on lazy evaluation          |
| Separation from algorithm | Yes                                | No                                          |
| Composability             | Yes (strategies)                   | Only building a parallel skeleton           |
| Implementation            | Part of GHC                        | A library, can choose scheduling algorithms |
| Speculative parallelism   | Yes                                | No (sparks are always executed)             |
| ThreadScope integration   | Yes                                | No                                          |
| Overhead                  | Lower, better at low granularities | Higher                                      |
| Run method                | Almost free runEval                | Expensive (avoid nested runPar calls)p      |

{{{PB}}}

* Concurrent Haskell

** Threads and MVars

*** Threads

*** Mvars

*** MVar as a simple channel

*** MVar as a container for shared state

*** MVar as a Building Block: Unbounded Channels

*** Fairness
